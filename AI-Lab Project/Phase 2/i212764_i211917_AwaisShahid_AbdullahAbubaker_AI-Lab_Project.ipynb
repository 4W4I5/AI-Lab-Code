{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import audioflux as af\n",
    "import IPython.display as ipd\n",
    "import joblib\n",
    "import librosa\n",
    "import librosa.display\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(fname):\n",
    "    y, sr = librosa.load(path=fname, sr=16_000, mono=True)\n",
    "\n",
    "    # Preprocessing i.e. Cut silence at start/end, remove bg noise.\n",
    "    y_trim, _ = librosa.effects.trim(y=y, top_db=10)\n",
    "    noise_reduced = nr.reduce_noise(y=y_trim, sr=sr)\n",
    "\n",
    "    # Onset detection\n",
    "    onsets = librosa.onset.onset_detect(y=noise_reduced, sr=sr, hop_length=128)\n",
    "    numberOfWords = len(onsets)\n",
    "\n",
    "    # Length of audio file\n",
    "    duration = len(y_trim) / sr\n",
    "\n",
    "    # Fundamental frequency extraction\n",
    "    f0, _, _ = librosa.pyin(y=y_trim, sr=sr, fmin=10, fmax=8000, frame_length=1024)\n",
    "    f0_values = [\n",
    "        np.nanmean(f0),\n",
    "        np.nanmedian(f0),\n",
    "        np.nanstd(f0),\n",
    "        np.nanpercentile(f0, 5),\n",
    "        np.nanpercentile(f0, 95),\n",
    "    ]\n",
    "\n",
    "    # Additional features\n",
    "    hnr = librosa.effects.harmonic(y=y_trim)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y_trim, sr=sr)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y_trim, sr=sr)[0]\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y_trim, sr=sr)[0]\n",
    "\n",
    "    # Send an update of the file being completed\n",
    "    print(f\"\\r[Parallelized]: File {fname[-17:]} completed\")\n",
    "\n",
    "    # Update metadata\n",
    "    return {\n",
    "        \"number_of_words\": numberOfWords,\n",
    "        \"duration\": duration,\n",
    "        \"words_per_second\": numberOfWords / duration,\n",
    "        \"pitch\": f0_values[0],\n",
    "        \"f0_median\": f0_values[1],\n",
    "        \"f0_std\": f0_values[2],\n",
    "        \"f0_5th_percentile\": f0_values[3],\n",
    "        \"f0_95th_percentile\": f0_values[4],\n",
    "        \"hnr\": np.mean(hnr),\n",
    "        \"spectral_centroid\": np.mean(spectral_centroid),\n",
    "        \"spectral_bandwidth\": np.mean(spectral_bandwidth),\n",
    "        \"spectral_contrast\": np.mean(spectral_contrast),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preProcess=False, limitFiles=None):\n",
    "    train_metadata = pd.read_csv(\"./Dataset/truncated_train.csv\")\n",
    "    test_metadata = pd.read_csv(\"./Dataset/cv-valid-test.csv\")\n",
    "    print(f\"[Main] Train metadata shape: {train_metadata.shape}\")\n",
    "    print(f\"[Main] Test metadata shape: {test_metadata.shape}\")\n",
    "    print(f\"[Main] Training Metadata: \\n {train_metadata.head()}\")\n",
    "    print(f\"[Main] Testing Metadata: \\n {test_metadata.head()}\")\n",
    "\n",
    "    # Drop duration column as it is filled later on\n",
    "    train_metadata.drop(columns=[\"duration\"], inplace=True)\n",
    "    test_metadata.drop(columns=[\"duration\"], inplace=True)\n",
    "\n",
    "    # remove rows that have a NaN value in ages column in training data\n",
    "    train_metadata = train_metadata[train_metadata[\"age\"].notna()].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\\n[Main] Train metadata shape after removing NaN values: {train_metadata.shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[Main] Train metadata after removing NaN values: \\n {train_metadata.head()}\"\n",
    "    )\n",
    "    age_mapping = {\n",
    "        \"teens\": 0,\n",
    "        \"twenties\": 1,\n",
    "        \"thirties\": 2,\n",
    "        \"fourties\": 3,\n",
    "        \"fifties\": 4,\n",
    "        \"sixties\": 5,\n",
    "        \"seventies\": 6,\n",
    "        \"eighties\": 7,\n",
    "        \"nineties\": 8,\n",
    "    }\n",
    "    train_metadata[\"age\"] = train_metadata[\"age\"].map(age_mapping)\n",
    "    test_metadata[\"age\"] = test_metadata[\"age\"].map(age_mapping)\n",
    "\n",
    "    # Remap genders, 0 is male 1 is female\n",
    "    gender_mapping = {\"male\": 0, \"female\": 1}\n",
    "    train_metadata[\"gender\"] = train_metadata[\"gender\"].map(gender_mapping)\n",
    "    test_metadata[\"gender\"] = test_metadata[\"gender\"].map(gender_mapping)\n",
    "\n",
    "    # Rename age to age_range\n",
    "    train_metadata.rename(columns={\"age\": \"age_range\"}, inplace=True)\n",
    "    test_metadata.rename(columns={\"age\": \"age_range\"}, inplace=True)\n",
    "    print(f\"\\n[Main] Train metadata after remapping: \\n {train_metadata.head()}\")\n",
    "    print(f\"\\n[Main] Test metadata after remapping: \\n {test_metadata.head()}\")\n",
    "\n",
    "    if preProcess == False:\n",
    "        # Preprocess the audio files\n",
    "        print(\"[Main] Preprocessing Training audio files...\")\n",
    "        prefix = \"./Dataset\"\n",
    "        # num_files = train_metadata.shape[0]\n",
    "        if limitFiles:\n",
    "            num_files = limitFiles\n",
    "        else:\n",
    "            num_files = train_metadata.shape[0]\n",
    "\n",
    "        # Parallelize preprocessing using joblib\n",
    "        processed_data = joblib.Parallel(n_jobs=-1)(\n",
    "            joblib.delayed(preprocess_audio)(\n",
    "                f\"{prefix}/{train_metadata['filename'][i]}\"\n",
    "            )\n",
    "            for i in range(num_files)\n",
    "        )\n",
    "\n",
    "        print(\"\\n[Main] Preprocessing Testing audio files...\")\n",
    "        # Parallize preprocessing using joblib for testing\n",
    "        processed_data_test = joblib.Parallel(n_jobs=-1)(\n",
    "            joblib.delayed(preprocess_audio)(f\"{prefix}/{test_metadata['filename'][i]}\")\n",
    "            for i in range(num_files)\n",
    "        )\n",
    "\n",
    "        # Update gender based on pitch, if pitch is greater than 165 then it is female, set it to 1, otherwise 0\n",
    "        for i, data in enumerate(processed_data):\n",
    "            if data[\"pitch\"] > 165:\n",
    "                processed_data[i][\"gender\"] = 1\n",
    "            else:\n",
    "                processed_data[i][\"gender\"] = 0\n",
    "\n",
    "        for i, data in enumerate(processed_data_test):\n",
    "            if data[\"pitch\"] > 165:\n",
    "                processed_data_test[i][\"gender\"] = 1\n",
    "            else:\n",
    "                processed_data_test[i][\"gender\"] = 0\n",
    "\n",
    "        # Normalize any NaN values in processed data with mean of the column\n",
    "        for i, data in enumerate(processed_data):\n",
    "            for key, value in data.items():\n",
    "                if np.isnan(value):\n",
    "                    processed_data[i][key] = train_metadata[key].mean()\n",
    "\n",
    "        for i, data in enumerate(processed_data_test):\n",
    "            for key, value in data.items():\n",
    "                if np.isnan(value):\n",
    "                    processed_data_test[i][key] = test_metadata[key].mean()\n",
    "\n",
    "        # Update metadata with preprocessed features\n",
    "        for i, data in enumerate(processed_data):\n",
    "            for key, value in data.items():\n",
    "                train_metadata.loc[i, key] = value\n",
    "\n",
    "        for i, data in enumerate(processed_data_test):\n",
    "            for key, value in data.items():\n",
    "                test_metadata.loc[i, key] = value\n",
    "\n",
    "        # Add duration column from processed_data to test metadata\n",
    "        for i, data in enumerate(processed_data):\n",
    "            test_metadata.loc[i, \"duration\"] = data[\"duration\"]\n",
    "\n",
    "        for i, data in enumerate(processed_data_test):\n",
    "            test_metadata.loc[i, \"duration\"] = data[\"duration\"]\n",
    "\n",
    "        # Write preprocessed metadata to file\n",
    "        train_metadata.to_csv(\n",
    "            \"./Dataset/preprocessed/trainingPreprocessed.csv\", index=False\n",
    "        )\n",
    "        test_metadata.to_csv(\n",
    "            \"./Dataset/preprocessed/testingPreprocessed.csv\", index=False\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Load preprocessed dataframes\n",
    "        train_metadata = pd.read_csv(\"./Dataset/preprocessed/trainingPreprocessed.csv\")\n",
    "        test_metadata = pd.read_csv(\"./Dataset/preprocessed/testingPreprocessed.csv\")\n",
    "\n",
    "    return train_metadata, test_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(train_metadata, test_metadata):\n",
    "    # Select target\n",
    "    target = \"age_range\"\n",
    "    y_train = train_metadata[target].values\n",
    "    y_test = test_metadata[target].values\n",
    "\n",
    "    # Drop filename and text columns\n",
    "    train_metadata.drop(columns=[\"filename\", \"text\", \"accent\"], inplace=True)\n",
    "    test_metadata.drop(columns=[\"filename\", \"text\", \"accent\"], inplace=True)\n",
    "\n",
    "    sns.heatmap(train_metadata.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    sns.heatmap(test_metadata.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "\n",
    "    # Select relevant features from the dataframe, ensure filename is dropped\n",
    "    selected_features = [\n",
    "        \"gender\",\n",
    "        \"number_of_words\",\n",
    "        \"words_per_second\",\n",
    "        \"pitch\",\n",
    "        \"f0_median\",\n",
    "        \"f0_std\",\n",
    "        \"f0_5th_percentile\",\n",
    "        \"f0_95th_percentile\",\n",
    "        \"hnr\",\n",
    "        \"spectral_centroid\",\n",
    "        \"spectral_bandwidth\",\n",
    "        \"spectral_contrast\",\n",
    "    ]\n",
    "\n",
    "    # Training set, only include rows that do not have a Nan value in any column\n",
    "    train_metadata = train_metadata.dropna()\n",
    "    test_metadata = test_metadata.dropna()\n",
    "\n",
    "    x_train = train_metadata[selected_features]\n",
    "    y_train = train_metadata[target]\n",
    "\n",
    "    # Testing set\n",
    "    x_test = test_metadata[selected_features]\n",
    "    y_test = test_metadata[target]\n",
    "\n",
    "    print(f\"[Main] Training set size: {x_train.shape}\")\n",
    "    print(f\"[Main] Testing set size: {x_test.shape}\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLinearRegression(x_train, y_train, x_test, y_test):\n",
    "    # Initialize Linear Regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Predict on the training and testing data\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"\\n[Linear Regression Model]\")\n",
    "    print(f\"\\tTrain MSE (Mean-Squared Err): {train_mse}\")\n",
    "    print(f\"\\tTest MSE (Mean-Squared Err): {test_mse}\")\n",
    "    print(f\"\\tTrain MAE (Mean-Absolute Err): {np.mean(np.abs(y_train - y_train_pred))}\")\n",
    "    print(f\"\\tTest MAE (Mean-Absolute Err): {np.mean(np.abs(y_test - y_test_pred))}\")\n",
    "    print(f\"\\tTrain R2 Score: {model.score(x_train, y_train)}\")\n",
    "    print(f\"\\tTest R2 Score: {model.score(x_test, y_test)}\")\n",
    "\n",
    "    print(f\"\\n[Linear Regression Model] Results:\")\n",
    "    if model.score(x_test, y_test) > 0.5:\n",
    "        print(f\"\\tModel is good\")\n",
    "    elif model.score(x_test, y_test) > 0.7:\n",
    "        print(f\"\\tModel is excellent\")\n",
    "    elif model.score(x_test, y_test) < 0:\n",
    "        print(f\"\\tModel is bad, possible overfitting\")\n",
    "    # print(f\"]tAccuracy Score: {accuracy_score(y_test, y_test_pred)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Check if the dataset is already preprocessed\n",
    "    if not os.path.exists(\"./Dataset/preprocessed\"):\n",
    "        os.makedirs(\"./Dataset/preprocessed\")\n",
    "        # print(f\"set preprocessed to False in first\")\n",
    "        preprocessed = False\n",
    "    else:\n",
    "        # Check if the preprocessed files are present\n",
    "        if not os.path.exists(\n",
    "            \"./Dataset/preprocessed/trainingPreprocessed.csv\"\n",
    "        ) or not os.path.exists(\"./Dataset/preprocessed/testingPreprocessed.csv\"):\n",
    "            # print(f\"set preprocessed to False in second\")\n",
    "            preprocessed = False\n",
    "        else:\n",
    "            # print(f\"set preprocessed to True in second\")\n",
    "            preprocessed = True\n",
    "\n",
    "    train_metadata, test_metadata = load_data(preProcess=preprocessed)\n",
    "    print(f\"[Main] Training data: \\n {train_metadata.head()}\")\n",
    "    print(f\"[Main] Testing data: \\n {test_metadata.head()}\")\n",
    "    x_train, y_train, x_test, y_test = splitData(train_metadata, test_metadata)\n",
    "\n",
    "    LinearRegressionModel = customLinearRegression(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    time_end = time.time()\n",
    "\n",
    "    print(\n",
    "        f\"\\n[Main] Program Execution Complete\\n\\tTime taken: {time_end - time_start:.2f} seconds.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
